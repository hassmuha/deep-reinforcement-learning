{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from matplotlib) (1.18.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: setuptools in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (46.1.1.post20200323)\n",
      "Requirement already satisfied: six in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "!pip install matplotlib\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 20x20 here\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=4, stride=4)\n",
    "        self.size=1*20*20\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc = nn.Linear(self.size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "        x = F.relu(self.conv(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        return self.sig(self.fc(x))\n",
    "\n",
    "\n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3702a4fab9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# try to add the option \"preprocess=pong_utils.preprocess_single\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to see what the agent sees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0manimate_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36manimate_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     47\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# play a game and display the animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    }
   ],
   "source": [
    "pong_utils.play(env, policy, time=200) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # prevents policy to become exactly 0 or 1 helps exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                                          epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1776c26255f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0manimate_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36manimate_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     47\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# play a game and display the animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    }
   ],
   "source": [
    "pong_utils.play(env, policy, time=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
