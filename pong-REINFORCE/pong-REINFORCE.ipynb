{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!\n",
    "\n",
    "(I strongly recommend you to try this notebook on the Udacity workspace first before running it locally on your desktop/laptop, as performance might suffer in different environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in /Users/hassan/miniconda3/envs/drlnd/lib/python3.6/site-packages (0.1)\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "#!pip install gym\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 20x20 here\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=4, stride=4)\n",
    "        self.size=1*20*20\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc = nn.Linear(self.size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "        x = F.relu(self.conv(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        return self.sig(self.fc(x))\n",
    "\n",
    "# use your own policy!\n",
    "# policy=Policy().to(device)\n",
    "\n",
    "\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-15b5b4d1e0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# try to add the option \"preprocess=pong_utils.preprocess_single\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to see what the agent sees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0manimate_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36manimate_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     47\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# play a game and display the animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    }
   ],
   "source": [
    "pong_utils.play(env, policy, time=100) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = pong_utils.parallelEnv('PongDeterministic-v4', n=4, seed=12345)\n",
    "prob, state, action, reward = pong_utils.collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([-1., -1., -1., -1.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([-1., -1., -1., -1.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([-1., -1., -1., -1.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([-1., -1., -1., -1.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([-1., -1., -1., -1.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0069, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    \n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(torch.tensor(old_probs)+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-torch.tensor(old_probs)+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n",
    "\n",
    "\n",
    "Lsur= surrogate(policy, prob, state, action, reward)\n",
    "\n",
    "print(Lsur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12074 sha256=b7a66ca5ecaaa5364d08b2be3021efde941041521d2636320f7168b2f09230f3\n",
      "  Stored in directory: /Users/hassan/Library/Caches/pip/wheels/bc/13/93/a9bf6b3d3966e4af014b0dbef027fdea47393faf47e990349f\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                          | ETA:  0:55:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -12.625000\n",
      "[-15. -13. -16.  -9. -16. -14.  -8. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                        | ETA:  0:52:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -15.125000\n",
      "[-16. -16. -16. -14. -14. -16. -15. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:50:26\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: -15.000000\n",
      "[-13. -17. -13. -14. -16. -16. -15. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                     | ETA:  0:47:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: -14.625000\n",
      "[-14. -16. -14. -16. -15. -16. -16. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:45:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: -13.625000\n",
      "[-11. -10. -15. -12. -16. -13. -16. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                 | ETA:  0:43:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: -13.750000\n",
      "[-14. -16. -13. -15. -17. -12. -11. -12.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |############                               | ETA:  0:40:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: -13.125000\n",
      "[-16. -12. -14. -11.  -8. -16. -14. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  0:38:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: -13.625000\n",
      "[-10. -16. -15. -12. -14. -11. -16. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  0:37:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: -13.250000\n",
      "[ -8. -12. -13. -13. -16. -16. -12. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:34:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: -14.000000\n",
      "[-16. -16. -13. -15. -11. -15. -10. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  44% |##################                         | ETA:  0:32:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: -13.875000\n",
      "[-16. -16. -16. -16. -10. -12. -15. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |####################                       | ETA:  0:30:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: -15.000000\n",
      "[-16. -15. -15. -11. -15. -16. -16. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |######################                     | ETA:  0:27:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: -13.875000\n",
      "[-13. -16. -13. -13. -16. -15. -10. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  0:25:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: -14.250000\n",
      "[-15. -11. -16. -16. -16. -12. -15. -13.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:23:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: -15.125000\n",
      "[-14. -16. -16. -15. -13. -16. -15. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  64% |###########################                | ETA:  0:20:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: -13.250000\n",
      "[-15. -14. -17.  -9. -16. -16.  -6. -13.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  68% |#############################              | ETA:  0:18:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: -13.500000\n",
      "[-15. -13.  -7. -13. -14. -16. -14. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  72% |##############################             | ETA:  0:16:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: -14.625000\n",
      "[-16. -16. -16. -13. -15. -14. -12. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  76% |################################           | ETA:  0:13:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: -14.000000\n",
      "[-12. -13. -17. -16. -17. -12. -10. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:11:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: -14.125000\n",
      "[-16. -13. -16. -16. -16. -11. -16.  -9.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  84% |####################################       | ETA:  0:09:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: -13.250000\n",
      "[-13. -16. -13. -16. -12.  -8. -16. -12.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  88% |#####################################      | ETA:  0:07:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: -13.875000\n",
      "[-13. -15. -13. -16. -13. -13. -14. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  92% |#######################################    | ETA:  0:04:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: -12.375000\n",
      "[-13. -16. -11.  -8. -11. -14. -14. -12.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:02:20\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: -13.375000\n",
      "[-14. -10. -14. -16. -12. -10. -15. -16.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:58:40\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: -13.625000\n",
      "[ -9. -17. -14. -11. -15. -13. -16. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "# episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "beta = .01\n",
    "tmax = 320\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    \n",
    "    L = -pong_utils.surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b1d90fde400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# play game after training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0manimate_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepRL/exercises/deep-reinforcement-learning/pong-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36manimate_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     47\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# play a game and display the animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    }
   ],
   "source": [
    "# play game after training!\n",
    "pong_utils.play(env, policy, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'REINFORCE.policy')\n",
    "\n",
    "# load your policy if needed\n",
    "# policy = torch.load('REINFORCE.policy')\n",
    "\n",
    "# try and test out the solution!\n",
    "# policy = torch.load('PPO_solution.policy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
